{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import seaborn as sns\n",
    "import math\n",
    "# %matplotlib inline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats import uniform as sp_randFloat\n",
    "from scipy.stats import randint as sp_randInt\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "# from sklearn.impute import KNNImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn import tree\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import (RandomOverSampler, \n",
    "                                    SMOTE, \n",
    "                                    ADASYN)\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo={'recommendation_set_id':str, 'user_id':str, 'session_id':str, 'query_identifier':str,\n",
    "'query_word_count':float, 'query_char_count':float, 'query_detected_language':str,\n",
    "'query_document_id':str, 'document_language_provided':str, 'year_published':float,\n",
    "'number_of_authors':float, 'abstract_word_count':float, 'abstract_char_count':float,\n",
    "'abstract_detected_language':str, 'first_author_id':str,\n",
    "'num_pubs_by_first_author':float, 'organization_id':str, 'application_type':str,\n",
    "'item_type':str, 'request_received':str, 'hour_request_received':str,\n",
    "'response_delivered':str, 'rec_processing_time':float, 'app_version':str, 'app_lang':str,\n",
    "'user_os':str, 'user_os_version':str, 'user_java_version':str, 'user_timezone':str,\n",
    "'country_by_ip':str, 'timezone_by_ip':str, 'local_time_of_request':str,\n",
    "'local_hour_of_request':str, 'number_of_recs_in_set':float,\n",
    "'recommendation_algorithm_id_used':str, 'algorithm_class':str, 'cbf_parser':str,\n",
    "'search_title':str, 'search_keywords':str, 'search_abstract':str,\n",
    "'time_recs_recieved':str, 'time_recs_displayed':str, 'time_recs_viewed':str,\n",
    "'clicks':float, 'ctr':float,'set_clicked':float\n",
    "   }\n",
    "pars=['request_received', 'response_delivered','local_time_of_request','time_recs_recieved','time_recs_displayed','time_recs_viewed']\n",
    "\n",
    "df_w=pd.read_csv('tcdml1920-rec-click-pred--training.csv',na_values=[\"\\\\N\",\"nA\"], dtype=lo, parse_dates=pars)\n",
    "df_kag=pd.read_csv('tcdml1920-rec-click-pred--test.csv',na_values=[\"\\\\N\",\"nA\"], dtype=lo, parse_dates=pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning jabref - based on EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global dict_key\n",
    "# dict_key={}\n",
    "def process(df,run,dict_key):\n",
    "#     global dict_key\n",
    "#     dict_key={}\n",
    "    \n",
    "    df=df[df.organization_id=='1'] # for jabref\n",
    "    df=df[['query_word_count','query_char_count', 'query_detected_language', 'query_document_id','year_published',\n",
    "           'number_of_authors','abstract_word_count', 'abstract_char_count','first_author_id','num_pubs_by_first_author',\n",
    "           'request_received','hour_request_received','rec_processing_time','app_version', 'app_lang','user_os','user_timezone','country_by_ip',\n",
    "           'timezone_by_ip','local_hour_of_request','recommendation_algorithm_id_used', 'algorithm_class', 'cbf_parser',\n",
    "           'search_title', 'search_keywords','search_abstract','clicks','set_clicked']]\n",
    "\n",
    "\n",
    "    # query_word_count\n",
    "    # query_char_count\n",
    "    # query_detected_language\n",
    "#     df['query_detected_language'].fillna(df['query_detected_language'].mode()[0], inplace=True)\n",
    "\n",
    "\n",
    "    # query_document_id\n",
    "    df['query_doc_id_present']=df.query_document_id.isna()*1\n",
    "    df.drop(columns='query_document_id',inplace=True)\n",
    "\n",
    "    # year_published\n",
    "\n",
    "    df.drop(df[df.year_published >2019].index, inplace=True)\n",
    "    # df['yr']=df.year_published.map(lambda x: '1950' if x<1950 else '1950-1980' if x<1980 else '1980-2000' if x<2000 else '2000-2010' if x<2010 else '2010-2019' if x<2019 else 0)\n",
    "    # df.groupby('yr').set_clicked.mean()\n",
    "    # df.drop(columns='yr',inplace=True)\n",
    "#     df.year_published.fillna(df.year_published.min(),inplace=True) # because pos correlation between year and clicks\n",
    "\n",
    "    # number_of_authors\n",
    "#     df.number_of_authors.fillna(10,inplace=True) #nulls have same mean of set_clicked as 10 authors\n",
    "\n",
    "    # abstract_word_count\n",
    "#     df.abstract_word_count.fillna(df.abstract_word_count.median(),inplace=True)\n",
    "    # abstract_char_count\n",
    "#     df.abstract_char_count.fillna(df.abstract_char_count.median(),inplace=True)\n",
    "\n",
    "# first_author_id\n",
    "    \n",
    "    # num_pubs_by_first_author\n",
    "#     df.num_pubs_by_first_author.fillna(df.local_hour_of_request.median(),inplace=True) \n",
    "\n",
    "    # request_received\n",
    "\n",
    "#     df['request_received']=df['request_received'].astype('datetime64[D]')\n",
    "    df['day_of_week'] = df['request_received'].dt.day_name()\n",
    "    df['month'] = df['request_received'].dt.month\n",
    "    df['year']=df['request_received'].dt.year\n",
    "    df['month']=df.month.astype(str)\n",
    "    df.drop(columns=['request_received'],inplace=True)\n",
    "\n",
    "    # hour_request_received\n",
    "#     df['xhr_req_recvd'] = df.hour_request_received.map(lambda x:math.sin(2.0*math.pi*float(x)/24))\n",
    "#     df['yhr_req_recvd']  = df.hour_request_received.map(lambda x:math.cos(2.0*math.pi*float(x)/24))\n",
    "#     df.drop(columns='hour_request_received',inplace=True)\n",
    "    \n",
    "#     rec_processing_time\n",
    "    if run=='train':\n",
    "        df=df[df.rec_processing_time<25]\n",
    "    \n",
    "    df.drop(columns=['rec_processing_time'],inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # app_version\n",
    "#     df['app_version'].fillna(df['app_version'].mode()[0], inplace=True)\n",
    "\n",
    "#     app_lang\n",
    "#     df['app_lang'].fillna(df['app_lang'].mode()[0], inplace=True)\n",
    "\n",
    "    # user_os\n",
    "    # df[(df.user_os.isna()==False)&(df.user_os!='Not provided')].groupby('user_os').set_clicked.sum()\n",
    "#     df['user_os_linux']=df.user_os.map(lambda x: 1 if x=='Linux' else 0)\n",
    "#     df['user_os_Mac_OS_X']=df.user_os.map(lambda x: 1 if x=='Mac OS X' else 0)\n",
    "#     df['user_os_Not_provided']=df.user_os.map(lambda x: 1 if x=='Not provided' else 0)\n",
    "#     df['user_os_Windows_10']=df.user_os.map(lambda x: 1 if x=='Windows 10' else 0)\n",
    "#     df['user_os_Windows_7']=df.user_os.map(lambda x: 1 if x=='Windows 7' else 0)\n",
    "    df['user_os_Windows_8_1']=df.user_os.map(lambda x: 1 if x=='Windows 8.1' else 0)\n",
    "    df['user_os_provided']=df.user_os.map(lambda x: 1 if x else 0)\n",
    "    df.drop(columns='user_os',inplace=True)\n",
    "\n",
    "\n",
    "    #     user_timezone\n",
    "    df['user_timezone_present']=df.user_timezone.map(lambda x:1 if x else 0)\n",
    "    df['user_timezone_aus']=df.user_timezone.map(lambda x:1 if x=='Australia/Sydney' else 0)\n",
    "    df.drop(columns='user_timezone',inplace=True)\n",
    "\n",
    "    # country_by_ip\n",
    "#     df['country_by_ip'].fillna(df['country_by_ip'].mode()[0], inplace=True) #do after 'other' thingy \n",
    "\n",
    "#     timezone_by_ip\n",
    "    # local_hour_of_request\n",
    "#     df.local_hour_of_request.fillna(df.local_hour_of_request.mode()[0],inplace=True) \n",
    "#     df['xhr_local_req_recvd'] = df.local_hour_of_request.map(lambda x:math.sin(2.0*math.pi*float(x)/24))\n",
    "#     df['yhr_local_req_recvd']  = df.local_hour_of_request.map(lambda x:math.cos(2.0*math.pi*float(x)/24))\n",
    "#     df.drop(columns='local_hour_of_request',inplace=True)\n",
    "\n",
    "    # recommendation_algorithm_id_used\n",
    "#     df['recommendation_algorithm_id_used']=df['recommendation_algorithm_id_used'].astype(str)\n",
    "#     df['recommendation_algorithm_id_used'].fillna(df['recommendation_algorithm_id_used'].mode()[0], inplace=True)\n",
    "\n",
    "    # algorithm_class\n",
    "    # cbf_parser\n",
    "#     df['cbf_standard_QP']=df.cbf_parser.map(lambda x:1 if x=='standard_QP' else 0)\n",
    "#     df['cbf_edismax_QP']=df.cbf_parser.map(lambda x:1 if x=='cbf_edismax_QP' else 0)\n",
    "#     df['cbf_mlt_QP']=df.cbf_parser.map(lambda x:1 if x=='cbf_mlt_QP' else 0)\n",
    "    df['cbf_parser_used']=df.cbf_parser.map(lambda x: 1 if x else 0)\n",
    "    df.drop(columns=['cbf_parser'],inplace=True)\n",
    "\n",
    "    # search_title\n",
    "    # search_keywords\n",
    "    # set_clicked\n",
    "\n",
    "    \n",
    "    def convert_sparse_values(df, cols, threshold, replacement='other'):\n",
    "        for col in [cols]:\n",
    "            counts = df[col].value_counts()\n",
    "            to_convert = counts[counts <= threshold].index.values\n",
    "            dict_key[cols]=to_convert\n",
    "            df[col] = df[col].replace(to_convert, replacement)\n",
    "\n",
    "                \n",
    "    if run=='train':\n",
    "        convert_sparse_values(df,cols='query_detected_language', threshold=1000)\n",
    "        convert_sparse_values(df,cols='app_lang', threshold=500)\n",
    "        convert_sparse_values(df,cols='country_by_ip', threshold=150)\n",
    "        convert_sparse_values(df,cols='timezone_by_ip', threshold=500)\n",
    "        convert_sparse_values(df,cols='app_version', threshold=800)\n",
    "\n",
    "    else:\n",
    "        df['query_detected_language']=df.query_detected_language.map(lambda x: x if x in dict_key['query_detected_language'] else 'others')\n",
    "        df['app_lang']=df.app_lang.map(lambda x: x if x in dict_key['app_lang'] else 'others')\n",
    "        df['country_by_ip']=df.country_by_ip.map(lambda x: x if x in dict_key['country_by_ip'] else 'others')\n",
    "        df['timezone_by_ip']=df.timezone_by_ip.map(lambda x: x if x in dict_key['country_by_ip'] else 'others')\n",
    "        df['app_version']=df.app_version.map(lambda x: x if x in dict_key['app_version'] else 'others')\n",
    "        \n",
    "#     df['country_by_ip'].fillna(df['country_by_ip'].mode()[0], inplace=True)\n",
    "#     df.timezone_by_ip.fillna(df.timezone_by_ip.mode()[0],inplace=True) \n",
    "\n",
    "#     df.drop(columns=['year_published','number_of_authors','abstract_word_count', 'abstract_char_count',\n",
    "#                     'first_author_id','num_pubs_by_first_author'],inplace=True)\n",
    "    \n",
    "    df=df.drop(columns=['clicks','set_clicked']).merge(df[['clicks','set_clicked']], \n",
    "                                    on=df[['clicks','set_clicked']].index).drop(columns='key_0')\n",
    "    \n",
    "    return df,dict_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_p,d=process(df_w,run='train',dict_key={}) #train\n",
    "\n",
    "df_kag_p,d=process(df_kag,run='test',dict_key=d) #kaggle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### category encodering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  making function to try combinations of all type of encodings\n",
    "\n",
    "def encode_all(df,df2,encoder_to_use,handle_missing='return_nan'):  #handle_missing='value'/'return_nan'\n",
    "    encoders_used={}\n",
    "    for col in encoder_to_use:\n",
    "        encoders_used[col]={}\n",
    "\n",
    "        if encoder_to_use[col]['name']=='BackwardDifferenceEncoder':\n",
    "            encoder=ce.BackwardDifferenceEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing)\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder'\n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "        if encoder_to_use[col]['name']=='BaseNEncoder':\n",
    "            encoder=ce.BaseNEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing,base=encoder_to_use[col]['base']) \n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder'\n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "        if encoder_to_use[col]['name']=='BinaryEncoder':\n",
    "            encoder=ce.BinaryEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing)\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder'\n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "        if encoder_to_use[col]['name']=='CatBoostEncoder':\n",
    "            encoder=ce.CatBoostEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing,sigma=encoder_to_use[col]['sigma'],a=encoder_to_use[col]['a'])\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder'\n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "    #     if encoder_to_use[col]['name']=='HashingEncoder':\n",
    "    #         encoder=ce.HashingEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing)\n",
    "    #         encoder.fit(X=df,y=df['set_clicked'])\n",
    "    #         df=encoder.transform(df)\n",
    "    #         encoders_used[col]['encoder_type']='BackwardDifferenceEncoder'\n",
    "#             encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "        if encoder_to_use[col]['name']=='HelmertEncoder':\n",
    "            encoder=ce.HelmertEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing)\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder'\n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "        if encoder_to_use[col]['name']=='JamesSteinEncoder':\n",
    "            encoder=ce.JamesSteinEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing, model=encoder_to_use[col]['model'])\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder'\n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "        if encoder_to_use[col]['name']=='LeaveOneOutEncoder':\n",
    "            encoder=ce.LeaveOneOutEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing,sigma=encoder_to_use[col]['sigma'])\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder' \n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "        if encoder_to_use[col]['name']=='MEstimateEncoder':\n",
    "            encoder=ce.MEstimateEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing,randomized=encoder_to_use[col]['randomized'],sigma=encoder_to_use[col]['sigma'],m=encoder_to_use[col]['m'])\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder' \n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "        if encoder_to_use[col]['name']=='OneHotEncoder':\n",
    "            encoder=ce.OneHotEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing,use_cat_names=True)\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder'\n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "        if encoder_to_use[col]['name']=='OrdinalEncoder':\n",
    "            encoder=ce.OrdinalEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing)\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder' \n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "        if encoder_to_use[col]['name']=='SumEncoder':\n",
    "            encoder=ce.SumEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing)\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder'\n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "        if encoder_to_use[col]['name']=='PolynomialEncoder':\n",
    "            encoder=ce.PolynomialEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing)\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder'\n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "        if encoder_to_use[col]['name']=='TargetEncoder':\n",
    "            encoder=ce.TargetEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing,min_samples_leaf=encoder_to_use[col]['min_samples_leaf'], smoothing=encoder_to_use[col]['smoothing'])\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder' \n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "\n",
    "        if encoder_to_use[col]['name']=='WOEEncoder':\n",
    "            encoder=ce.WOEEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing,randomized=encoder_to_use[col]['randomized'],sigma=encoder_to_use[col]['sigma'])\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder' \n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "            \n",
    "    return df,df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either use above function to encode different columns differently or encode all using single encoder as below\n",
    "\n",
    "encoder=ce.WOEEncoder(randomized=True,sigma=0.1)\n",
    "encoder.fit(X=df_w_p.iloc[:,:-2],y=df_w_p['set_clicked'])\n",
    "df_w_encoded=encoder.transform(df_w_p.iloc[:,:-2])\n",
    "df_kag_encoded=encoder.transform(df_kag_p.iloc[:,:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delete unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_encoded.drop(columns=['algorithm_class','search_title','search_keywords','search_abstract'],inplace=True)\n",
    "df_kag_encoded.drop(columns=['algorithm_class','search_title','search_keywords','search_abstract'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "est=IterativeImputer(random_state=0, estimator= KNeighborsRegressor(n_neighbors=10),n_nearest_features=6)\n",
    "est=IterativeImputer(random_state=0, initial_strategy='median')\n",
    "est.fit(df_w_encoded)\n",
    "X_w_im=est.transform(df_w_encoded)\n",
    "X_kag_im=est.transform(df_kag_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove set_clicked and clicks from feature data\n",
    "X_w=X_w_im[:,:-2]\n",
    "X_kag=X_kag_im[:,:-2]\n",
    "\n",
    "y_w=df_w_p.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df_w_p.drop(columns=['set_clicked']).values\n",
    "# y = df_w_p.loc[:, 'set_clicked'].values\n",
    "\n",
    "# df_x=df_w_p.drop(columns=['set_clicked'])\n",
    "# df_y=df_w_p.loc[:, 'set_clicked']\n",
    "\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_w,y_w, test_size = 0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oversampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import (RandomOverSampler, \n",
    "                                    SMOTE, \n",
    "                                    ADASYN)\n",
    "# RandomOverSampler\n",
    "  # With over-sampling methods, the number of samples in a class\n",
    "  # should be greater or equal to the original number of samples.\n",
    "# sampler = RandomOverSampler(ratio={1: 1927, 0: 300},random_state=0)\n",
    "# X_rs, y_rs = sampler.fit_sample(X, y)\n",
    "# print('RandomOverSampler {}'.format(Counter(y_rs)))\n",
    "# plot_this(X_rs,y_rs)\n",
    "# SMOTE\n",
    "# sampler = SMOTE(ratio=0.4)\n",
    "# X_train, y_train = sampler.fit_sample(X_train, y_train)\n",
    "# print('SMOTE {}'.format(Counter(y_rs)))\n",
    "# plot_this(X_rs,y_rs)\n",
    "# ADASYN;n_neighbors=9,\n",
    "# sampler = ADASYN(ratio={1: 1927, 0: 300},random_state=0)\n",
    "# X_rs, y_rs = sampler.fit_sample(X, y)\n",
    "# print('ADASYN {}'.format(Counter(y_rs)))\n",
    "# plot_this(X_rs,y_rs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### catboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = CatBoostClassifier(iterations=1000,learning_rate=.02,depth=4\n",
    "                                       ,task_type=\"GPU\",devices='0:2'\n",
    "                          )\n",
    "# Fit model\n",
    "model.fit(X_train, y_train,verbose=True,eval_set=(X_test,y_test))\n",
    "\n",
    "preds_class = model.predict(X_test)\n",
    "# Get predicted probabilities for each class\n",
    "preds_proba = model.predict_proba(X_test)\n",
    "\n",
    "print(\"\")\n",
    "# print(\"rate:\",rate/100,\"iters:\",iters,\"depth:\",depth)\n",
    "acc=sum(y_test==preds_class)/len(y_test)\n",
    "print(\"accuracy: \",acc)\n",
    "print(\"f1: \",f1_score(y_test, preds_class))\n",
    "print(\"cm:\",confusion_matrix(y_test, preds_class))\n",
    "\n",
    "print(\"kaggle: \",sum(model.predict(kag_del)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(clf.predict(kag_del)).to_csv('aa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paramGrid = {\"subsample\" : [1, 0.9, 0.5],\n",
    "             'depth'         : [3,5,8],\n",
    "                  'learning_rate' : [.2,.5,.9],\n",
    "                  'iterations'    : [500,1000,1500]\n",
    "                 \n",
    "            }\n",
    "\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "gridsearch = GridSearchCV(model, paramGrid, verbose=True,             \n",
    "         cv=TimeSeriesSplit(n_splits=3).get_n_splits([X_train, y_train]))\n",
    "\n",
    "gridsearch.fit(X_train, y_train,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true, y_pred = y_test, gridsearch.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preds=gridsearch.predict(X_kag)\n",
    "sum(gridsearch.predict(X_kag_p1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_class = clf.predict(X_test)\n",
    "acc=sum(y_test==preds_class)/len(y_test)\n",
    "print(\"accuracy: \",acc)\n",
    "print(\"f1: \",f1_score(y_test, preds_class))\n",
    "print(\"cm:\",confusion_matrix(y_test, preds_class))\n",
    "\n",
    "print(\"kaggle: \",sum(clf.predict(X_kag)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=5,random_state=0).fit(X_train, y_train)\n",
    "preds_class = model.predict(X_test)\n",
    "acc=sum(y_test==preds_class)/len(y_test)\n",
    "print(\"accuracy: \",acc)\n",
    "print(\"f1: \",f1_score(y_test, preds_class))\n",
    "print(\"cm:\",confusion_matrix(y_test, preds_class))\n",
    "\n",
    "print(\"kaggle: \",sum(clf.predict(kag_del)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decision tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ff=[]\n",
    "kag=[]\n",
    "for i in range(2,30,1):\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=i,).fit(X_train, y_train)\n",
    "\n",
    "    preds_class = clf.predict(X_test)\n",
    "    acc=sum(y_test==preds_class)/len(y_test)\n",
    "    print(i)\n",
    "    print(\"accuracy: \",acc)\n",
    "    print(\"f1: \",f1_score(y_test, preds_class))\n",
    "    print(\"cm:\",confusion_matrix(y_test, preds_class))\n",
    "\n",
    "    print(\"kaggle: \",sum(clf.predict(X_kag)))\n",
    "    ff.append(f1_score(y_test, preds_class))\n",
    "    kag.append(sum(clf.predict(X_kag)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# Neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim=52, activation='relu'))\n",
    "# model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train,validation_data = (X_test,y_test), epochs=10, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_class = model.predict(X_test).ravel()\n",
    "acc=sum(y_test==preds_class)/len(y_test)\n",
    "print(i)\n",
    "print(\"accuracy: \",acc)\n",
    "print(\"f1: \",f1_score(y_test, preds_class))\n",
    "print(\"cm:\",confusion_matrix(y_test, preds_class))\n",
    "\n",
    "print(\"kaggle: \",sum(clf.predict(X_kag)))\n",
    "ff.append(f1_score(y_test, preds_class))\n",
    "kag.append(sum(clf.predict(X_kag)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trying different combination of encodings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'name':'BackwardDifferenceEncoder'},\n",
    "# {'name':'LeaveOneOutEncoder','sigma':.3}, #optimal value is commonly between 0.05 and 0.6\n",
    "# {'name':'BaseNEncoder','base':2},\n",
    "# {'name':'JamesSteinEncoder','model':'binary'},\n",
    "# {'name':'TargetEncoder','min_samples_leaf':10, 'smoothing':4.0},\n",
    "# {'name':'WOEEncoder','randomized':True, 'sigma':0.1},\n",
    "# {'name':'MEstimateEncoder','randomized':True, 'sigma':0.1,'m':2}, #default m=1\n",
    "# {'name':'CatBoostEncoder','sigma':None,'a':2}, #default m=1\n",
    "\n",
    "\n",
    "encoders_to_use=[\n",
    "{\n",
    "'query_detected_language':{'name':'TargetEncoder','min_samples_leaf':10, 'smoothing':4.0},\n",
    "'first_author_id':{'name':'TargetEncoder','min_samples_leaf':10, 'smoothing':4.0},\n",
    "'hour_request_received':{'name':'BaseNEncoder','base':2},\n",
    "'app_version':{'name':'TargetEncoder','min_samples_leaf':10, 'smoothing':4.0},\n",
    "'app_lang':{'name':'TargetEncoder','min_samples_leaf':10, 'smoothing':4.0},\n",
    "'country_by_ip':{'name':'TargetEncoder','min_samples_leaf':10, 'smoothing':4.0},\n",
    "'timezone_by_ip':{'name':'TargetEncoder','min_samples_leaf':10, 'smoothing':4.0},\n",
    "'local_hour_of_request':{'name':'BaseNEncoder','base':2},\n",
    "'recommendation_algorithm_id_used':{'name':'BaseNEncoder','base':2},\n",
    "'day_of_week':{'name':'BaseNEncoder','base':2},\n",
    "'month':{'name':'BaseNEncoder','base':2}\n",
    "},\n",
    "    {\n",
    "'query_detected_language':{'name':'MEstimateEncoder','randomized':True, 'sigma':0.1,'m':2},\n",
    "'first_author_id':{'name':'MEstimateEncoder','randomized':True, 'sigma':0.1,'m':2},\n",
    "'hour_request_received':{'name':'BaseNEncoder','base':2},\n",
    "'app_version':{'name':'MEstimateEncoder','randomized':True, 'sigma':0.1,'m':2},\n",
    "'app_lang':{'name':'MEstimateEncoder','randomized':True, 'sigma':0.1,'m':2},\n",
    "'country_by_ip':{'name':'MEstimateEncoder','randomized':True, 'sigma':0.1,'m':2},\n",
    "'timezone_by_ip':{'name':'MEstimateEncoder','randomized':True, 'sigma':0.1,'m':2},\n",
    "'local_hour_of_request':{'name':'BaseNEncoder','base':2},\n",
    "'recommendation_algorithm_id_used':{'name':'BaseNEncoder','base':2},\n",
    "'day_of_week':{'name':'BaseNEncoder','base':2},\n",
    "'month':{'name':'BaseNEncoder','base':2}\n",
    "},\n",
    "    {\n",
    "'query_detected_language':{'name':'LeaveOneOutEncoder','sigma':.3},\n",
    "'first_author_id':{'name':'LeaveOneOutEncoder','sigma':.3},\n",
    "'hour_request_received':{'name':'BaseNEncoder','base':2},\n",
    "'app_version':{'name':'LeaveOneOutEncoder','sigma':.3},\n",
    "'app_lang':{'name':'LeaveOneOutEncoder','sigma':.3},\n",
    "'country_by_ip':{'name':'LeaveOneOutEncoder','sigma':.3},\n",
    "'timezone_by_ip':{'name':'LeaveOneOutEncoder','sigma':.3},\n",
    "'local_hour_of_request':{'name':'BaseNEncoder','base':2},\n",
    "'recommendation_algorithm_id_used':{'name':'BaseNEncoder','base':2},\n",
    "'day_of_week':{'name':'BaseNEncoder','base':2},\n",
    "'month':{'name':'BaseNEncoder','base':2}\n",
    "},\n",
    "    {\n",
    "'query_detected_language':{'name':'LeaveOneOutEncoder','sigma':.1},\n",
    "'first_author_id':{'name':'LeaveOneOutEncoder','sigma':.1},\n",
    "'hour_request_received':{'name':'BaseNEncoder','base':2},\n",
    "'app_version':{'name':'LeaveOneOutEncoder','sigma':.1},\n",
    "'app_lang':{'name':'LeaveOneOutEncoder','sigma':.1},\n",
    "'country_by_ip':{'name':'LeaveOneOutEncoder','sigma':.1},\n",
    "'timezone_by_ip':{'name':'LeaveOneOutEncoder','sigma':.1},\n",
    "'local_hour_of_request':{'name':'BaseNEncoder','base':2},\n",
    "'recommendation_algorithm_id_used':{'name':'BaseNEncoder','base':2},\n",
    "'day_of_week':{'name':'BaseNEncoder','base':2},\n",
    "'month':{'name':'BaseNEncoder','base':2}\n",
    "},\n",
    "    {\n",
    "'query_detected_language':{'name':'WOEEncoder','randomized':True, 'sigma':0.1},\n",
    "'first_author_id':{'name':'WOEEncoder','randomized':True, 'sigma':0.1},\n",
    "'hour_request_received':{'name':'BaseNEncoder','base':2},\n",
    "'app_version':{'name':'WOEEncoder','randomized':True, 'sigma':0.1},\n",
    "'app_lang':{'name':'WOEEncoder','randomized':True, 'sigma':0.1},\n",
    "'country_by_ip':{'name':'WOEEncoder','randomized':True, 'sigma':0.1},\n",
    "'timezone_by_ip':{'name':'WOEEncoder','randomized':True, 'sigma':0.1},\n",
    "'local_hour_of_request':{'name':'BaseNEncoder','base':2},\n",
    "'recommendation_algorithm_id_used':{'name':'BaseNEncoder','base':2},\n",
    "'day_of_week':{'name':'BaseNEncoder','base':2},\n",
    "'month':{'name':'BaseNEncoder','base':2}\n",
    "},\n",
    "    {\n",
    "'query_detected_language':{'name':'WOEEncoder','randomized':True, 'sigma':0.3},\n",
    "'first_author_id':{'name':'WOEEncoder','randomized':True, 'sigma':0.3},\n",
    "'hour_request_received':{'name':'BaseNEncoder','base':2},\n",
    "'app_version':{'name':'WOEEncoder','randomized':True, 'sigma':0.3},\n",
    "'app_lang':{'name':'WOEEncoder','randomized':True, 'sigma':0.3},\n",
    "'country_by_ip':{'name':'WOEEncoder','randomized':True, 'sigma':0.3},\n",
    "'timezone_by_ip':{'name':'WOEEncoder','randomized':True, 'sigma':0.3},\n",
    "'local_hour_of_request':{'name':'BaseNEncoder','base':2},\n",
    "'recommendation_algorithm_id_used':{'name':'BaseNEncoder','base':2},\n",
    "'day_of_week':{'name':'BaseNEncoder','base':2},\n",
    "'month':{'name':'BaseNEncoder','base':2}\n",
    "}\n",
    "]\n",
    "    \n",
    "encoders_used={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding:  0\n",
      "cols:  (267537, 33)\n",
      "i:  0\n",
      "imputing...\n",
      "dtree...\n",
      "f1:  0.006134969325153374\n",
      "cat...\n",
      "f1:  0.01676829268292683\n",
      "RF...\n",
      "f1:  0.0\n",
      "i:  1\n",
      "imputing...\n",
      "dtree...\n",
      "f1:  0.003427592116538132\n",
      "cat...\n"
     ]
    }
   ],
   "source": [
    "ff=[]\n",
    "kag=[]\n",
    "\n",
    "for xx,encoder_to_use in enumerate(encoders_to_use):\n",
    "    print(\"encoding: \",xx)\n",
    "    df_w_encoded, df_kag_encoded=encode_all(df_w_p.copy(), df_kag_p.copy(),encoder_to_use)\n",
    "\n",
    "    df_w_encoded.drop(columns=['algorithm_class','search_title','search_keywords','search_abstract','year_published',\n",
    "                                'number_of_authors',\n",
    "                                'abstract_word_count',\n",
    "                                'abstract_char_count',\n",
    "                                'first_author_id',\n",
    "                                'num_pubs_by_first_author'],inplace=True)\n",
    "    df_kag_encoded.drop(columns=['algorithm_class','search_title','search_keywords','search_abstract','year_published',\n",
    "                                'number_of_authors',\n",
    "                                'abstract_word_count',\n",
    "                                'abstract_char_count',\n",
    "                                'first_author_id',\n",
    "                                'num_pubs_by_first_author'],inplace=True)\n",
    "    print(\"cols: \",df_w_encoded.shape)\n",
    "    for i in range(5):\n",
    "        print(\"i: \",i)\n",
    "        print(\"imputing...\")\n",
    "\n",
    "        est=IterativeImputer(random_state=0, estimator= None,initial_strategy='mean')\n",
    "        est.fit(df_w_encoded)\n",
    "        X_w_im=est.transform(df_w_encoded)\n",
    "        X_kag_im=est.transform(df_kag_encoded)\n",
    "\n",
    "        X_w=X_w_im[:,:-2]\n",
    "        X_kag=X_kag_im[:,:-2]\n",
    "        y_w=df_w_p.iloc[:,-1]\n",
    "\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_w,y_w, test_size = random.randrange(10,30)/100)\n",
    "            \n",
    "\n",
    "    #     DTREE\n",
    "        print(\"dtree...\")\n",
    "        clf = tree.DecisionTreeClassifier(max_depth=random.randrange(4,5)).fit(X_train, y_train)\n",
    "\n",
    "        preds_class = clf.predict(X_test)\n",
    "#         acc=sum(y_test==preds_class)/len(y_test)\n",
    "#         print(i)\n",
    "    #         print(\"accuracy: \",acc)\n",
    "        print(\"f1: \",f1_score(y_test, preds_class))\n",
    "    #         print(\"cm:\",confusion_matrix(y_test, preds_class))\n",
    "\n",
    "#         print(\"kaggle: \",sum(clf.predict(X_kag)))\n",
    "#         ff.append(f1_score(y_test, preds_class))\n",
    "        kag.append(list(clf.predict(X_kag)))\n",
    "\n",
    "\n",
    "    # CAT\n",
    "        print(\"cat...\")\n",
    "        model = CatBoostClassifier(iterations=2000,learning_rate=.02,depth=random.randrange(4,5)\n",
    "                                               ,task_type=\"GPU\",devices='0:2'\n",
    "                                  )\n",
    "        # Fit model\n",
    "        model.fit(X_train, y_train,verbose=False,eval_set=(X_test,y_test))\n",
    "\n",
    "        preds_class = model.predict(X_test)\n",
    "#         print(\"\")\n",
    "        # print(\"rate:\",rate/100,\"iters:\",iters,\"depth:\",depth)\n",
    "    #     acc=sum(y_test==preds_class)/len(y_test)\n",
    "    #     print(\"accuracy: \",acc)\n",
    "        print(\"f1: \",f1_score(y_test, preds_class))\n",
    "    #     print(\"cm:\",confusion_matrix(y_test, preds_class))\n",
    "\n",
    "#         print(\"kaggle: \",sum(model.predict(X_kag)))\n",
    "#         ff.append(f1_score(y_test, preds_class))\n",
    "        kag.append(list(clf.predict(X_kag)))\n",
    "\n",
    "#     RF\n",
    "        print('RF...')\n",
    "        clf = RandomForestClassifier(n_estimators=500, max_depth=random.randrange(4,5)).fit(X_train, y_train)\n",
    "        preds_class = clf.predict(X_test)\n",
    "#         acc=sum(y_test==preds_class)/len(y_test)\n",
    "#         print(\"accuracy: \",acc)\n",
    "        print(\"f1: \",f1_score(y_test, preds_class))\n",
    "#         print(\"cm:\",confusion_matrix(y_test, preds_class))\n",
    "\n",
    "#         print(\"kaggle: \",sum(clf.predict(kag_del)))\n",
    "        kag.append(list(clf.predict(X_kag)))\n",
    "\n",
    "pd.DataFrame(kag).to_csv('aa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model1...\n",
      "training model2...\n",
      "training model3...\n",
      "training model4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nishant\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_w, y_w, test_size=0.4 )\n",
    "sampler = SMOTE(ratio=0.4)\n",
    "x_train, y_train = sampler.fit_sample(x_train, y_train)\n",
    "# x_train=pd.DataFrame(x_train).reset_index().iloc[:,1:].values\n",
    "# x_test=pd.DataFrame(x_test).reset_index().iloc[:,1:].values\n",
    "# y_train=pd.DataFrame(y_train).reset_index().iloc[:,1].values\n",
    "# y_test=pd.DataFrame(y_test).reset_index().iloc[:,1].values\n",
    "\n",
    "test_preds=[]\n",
    "kag_preds=[]\n",
    "\n",
    "print(\"training model1...\")\n",
    "model1 = tree.DecisionTreeClassifier(max_depth=4)\n",
    "model1.fit(x_train,y_train)\n",
    "train_pred1=model1.predict(x_train)\n",
    "test_pred1=model1.predict(x_test)\n",
    "kag_pred1=model1.predict(X_kag)\n",
    "test_preds.append(test_pred1)\n",
    "kag_preds.append(kag_pred1)\n",
    "\n",
    "print(\"training model2...\")\n",
    "model2 = CatBoostClassifier(iterations=1500,learning_rate=.02,depth=random.randrange(4,5)\n",
    "                                               ,task_type=\"GPU\",devices='0:2'\n",
    "                                  )\n",
    "model2.fit(x_train,y_train, eval_set=(x_test,y_test),verbose=False)\n",
    "train_pred2=model2.predict(x_train)\n",
    "test_pred2=model2.predict(x_test)\n",
    "kag_pred2=model2.predict(X_kag)\n",
    "test_preds.append(test_pred2)\n",
    "kag_preds.append(kag_pred2)\n",
    "\n",
    "print(\"training model3...\")\n",
    "model3 = RandomForestClassifier(n_estimators=1000)\n",
    "model3.fit(x_train,y_train)\n",
    "train_pred3=model3.predict(x_train)\n",
    "test_pred3=model3.predict(x_test)\n",
    "kag_pred3=model3.predict(X_kag)\n",
    "test_preds.append(test_pred3)\n",
    "kag_preds.append(kag_pred3)\n",
    "\n",
    "print(\"training model4...\")\n",
    "model4 = tree.DecisionTreeClassifier(max_depth=5)\n",
    "model4.fit(x_train,y_train)\n",
    "train_pred4=model4.predict(x_train)\n",
    "test_pred4=model4.predict(x_test)\n",
    "kag_pred4=model4.predict(X_kag)\n",
    "test_preds.append(test_pred4)\n",
    "kag_preds.append(kag_pred4)\n",
    "\n",
    "# stack\n",
    "test_df=pd.DataFrame(test_preds).T\n",
    "kag_df=pd.DataFrame(kag_preds).T\n",
    "\n",
    "model = LogisticRegression(random_state=1)\n",
    "model.fit(test_df,y_test)\n",
    "kag_preds=model.predict(kag_df)\n",
    "\n",
    "kag_preds.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nishant\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pd.Series(kag_preds).to_csv('aa.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\nishant\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\nishant\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\nishant\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\nishant\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\nishant\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\nishant\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "# import seaborn as sns\n",
    "import math\n",
    "# %matplotlib inline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats import uniform as sp_randFloat\n",
    "from scipy.stats import randint as sp_randInt\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "# from sklearn.impute import KNNImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn import tree\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import (RandomOverSampler, \n",
    "                                    SMOTE, \n",
    "                                    ADASYN)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lo={'recommendation_set_id':str, 'user_id':str, 'session_id':str, 'query_identifier':str,\n",
    "'query_word_count':float, 'query_char_count':float, 'query_detected_language':str,\n",
    "'query_document_id':str, 'document_language_provided':str, 'year_published':float,\n",
    "'number_of_authors':float, 'abstract_word_count':float, 'abstract_char_count':float,\n",
    "'abstract_detected_language':str, 'first_author_id':str,\n",
    "'num_pubs_by_first_author':float, 'organization_id':str, 'application_type':str,\n",
    "'item_type':str, 'request_received':str, 'hour_request_received':str,\n",
    "'response_delivered':str, 'rec_processing_time':float, 'app_version':str, 'app_lang':str,\n",
    "'user_os':str, 'user_os_version':str, 'user_java_version':str, 'user_timezone':str,\n",
    "'country_by_ip':str, 'timezone_by_ip':str, 'local_time_of_request':str,\n",
    "'local_hour_of_request':str, 'number_of_recs_in_set':float,\n",
    "'recommendation_algorithm_id_used':str, 'algorithm_class':str, 'cbf_parser':str,\n",
    "'search_title':str, 'search_keywords':str, 'search_abstract':str,\n",
    "'time_recs_recieved':str, 'time_recs_displayed':str, 'time_recs_viewed':str,\n",
    "'clicks':float, 'ctr':float,'set_clicked':float\n",
    "   }\n",
    "# pars=['request_received', 'response_delivered','local_time_of_request','time_recs_recieved','time_recs_displayed','time_recs_viewed']\n",
    "pars=['request_received']\n",
    "\n",
    "df_w=pd.read_csv('tcdml1920-rec-click-pred--training-myvolt.csv',na_values=[\"\\\\N\",\"nA\"], dtype=lo, parse_dates=pars)\n",
    "df_kag=pd.read_csv('tcdml1920-rec-click-pred--test.csv',na_values=[\"\\\\N\",\"nA\"], dtype=lo, parse_dates=pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning jabref "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global dict_key\n",
    "# dict_key={}\n",
    "def process(df,run,dict_key):\n",
    "#     global dict_key\n",
    "#     dict_key={}\n",
    "    \n",
    "    df=df[df.organization_id=='4']\n",
    "    df=df[[\n",
    "        'user_id',\n",
    "#         'session_id',\n",
    "        'query_word_count', 'query_detected_language',\n",
    "       'query_document_id','abstract_word_count','abstract_detected_language', \n",
    "       'item_type'\n",
    "#            ,'request_received'\n",
    "           ,'hour_request_received','country_by_ip', 'timezone_by_ip', 'local_hour_of_request',\n",
    "       'recommendation_algorithm_id_used','set_clicked']]\n",
    "\n",
    "# user_id\n",
    "#     df['user_id_present']=df.user_id.isna()*1\n",
    "#     df.drop(columns='user_id',inplace=True)\n",
    "    \n",
    "#     session_id\n",
    "#     df['session_id']=df.session_id.isna()*1\n",
    "#     df.drop(columns='session_id',inplace=True)\n",
    "    \n",
    "    # query_word_count\n",
    "    \n",
    "    # query_detected_language\n",
    "\n",
    "    # query_document_id\n",
    "#     df['query_doc_id_present']=df.query_document_id.isna()*1\n",
    "#     df.drop(columns='query_document_id',inplace=True)\n",
    "\n",
    "#     df['request_received']=df['request_received'].astype('datetime64[D]')\n",
    "#     df['day_of_week'] = df['request_received'].dt.day_name()\n",
    "#     df['month'] = df['request_received'].dt.month\n",
    "#     df['year']=df['request_received'].dt.year\n",
    "#     df['month']=df.month.astype(str)\n",
    "#     df.drop(columns=['request_received'],inplace=True)\n",
    "\n",
    "    # country_by_ip\n",
    "#     df['country_by_ip'].fillna(df['country_by_ip'].mode()[0], inplace=True) #do after 'other' thingy \n",
    "\n",
    "#     timezone_by_ip\n",
    "    # local_hour_of_request\n",
    "\n",
    "    # recommendation_algorithm_id_used\n",
    "\n",
    "    def convert_sparse_values(df, cols, threshold, replacement='other'):\n",
    "        for col in [cols]:\n",
    "            counts = df[col].value_counts()\n",
    "            to_convert = counts[counts <= threshold].index.values\n",
    "            dict_key[cols]=to_convert\n",
    "            df[col] = df[col].replace(to_convert, replacement)\n",
    "\n",
    "                \n",
    "#     if run=='train':\n",
    "#         convert_sparse_values(df,cols='query_detected_language', threshold=1000)\n",
    "#         convert_sparse_values(df,cols='app_lang', threshold=500)\n",
    "#         convert_sparse_values(df,cols='country_by_ip', threshold=150)\n",
    "#         convert_sparse_values(df,cols='timezone_by_ip', threshold=500)\n",
    "#         convert_sparse_values(df,cols='app_version', threshold=800)\n",
    "\n",
    "#     else:\n",
    "#         df['query_detected_language']=df.query_detected_language.map(lambda x: x if x in dict_key['query_detected_language'] else 'others')\n",
    "#         df['app_lang']=df.app_lang.map(lambda x: x if x in dict_key['app_lang'] else 'others')\n",
    "#         df['country_by_ip']=df.country_by_ip.map(lambda x: x if x in dict_key['country_by_ip'] else 'others')\n",
    "#         df['timezone_by_ip']=df.timezone_by_ip.map(lambda x: x if x in dict_key['country_by_ip'] else 'others')\n",
    "#         df['app_version']=df.app_version.map(lambda x: x if x in dict_key['app_version'] else 'others')\n",
    "        \n",
    "#     df['country_by_ip'].fillna(df['country_by_ip'].mode()[0], inplace=True)\n",
    "#     df.timezone_by_ip.fillna(df.timezone_by_ip.mode()[0],inplace=True) \n",
    "\n",
    "    df.drop(columns=['abstract_word_count'],inplace=True)\n",
    "    \n",
    "    df=df.drop(columns=['set_clicked']).merge(df[['set_clicked']], \n",
    "                                    on=df[['set_clicked']].index).drop(columns='key_0')\n",
    "    \n",
    "    return df,dict_key\n",
    "\n",
    "\n",
    "df_w_p,d=process(df_w,run='train',dict_key={}) #train\n",
    "\n",
    "df_kag_p,d=process(df_kag,run='test',dict_key=d) #kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kags=[]\n",
    "# for i in range(10):\n",
    "if 1==0:\n",
    "    \n",
    "    print(\"---------------------------\")\n",
    "    sig=random.randrange(1,20)/10\n",
    "#     print(\"sig\",sig)\n",
    "    encoder=ce.LeaveOneOutEncoder()\n",
    "    encoder.fit(X=df_w_p.iloc[:,:-1],y=df_w_p['set_clicked'])\n",
    "    df_w_encoded=encoder.transform(df_w_p.iloc[:,:-1])\n",
    "    df_kag_encoded=encoder.transform(df_kag_p.iloc[:,:-1])\n",
    "\n",
    "    df_w_encoded['set_clicked']=df_w_p.iloc[:,-1]\n",
    "    df_kag_encoded['set_clicked']=df_kag_p.iloc[:,-1]\n",
    "\n",
    "    sig=random.randrange(3,20)\n",
    "#     print(\"neighbors\",sig)\n",
    "    est=IterativeImputer(random_state=0, estimator= KNeighborsRegressor(n_neighbors=30))\n",
    "    est.fit(df_w_encoded)\n",
    "    X_w_im=est.transform(df_w_encoded)\n",
    "    X_kag_im=est.transform(df_kag_encoded)\n",
    "\n",
    "    X_w=X_w_im[:,:-1]\n",
    "    X_kag=X_kag_im[:,:-1]\n",
    "    y_w=df_w_p.iloc[:,-1]\n",
    "\n",
    "    sig=random.randrange(5,40)/100\n",
    "    print(\"split\",sig)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_w, y_w, test_size=sig, stratify=y_w)\n",
    "\n",
    "    sampler = SMOTE(ratio=0.2)\n",
    "#     x_train, y_train = sampler.fit_sample(x_train, y_train)\n",
    "\n",
    "    sig=random.randrange(3,8)\n",
    "    print(\"depth\",sig)\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=sig)\n",
    "    clf.fit(x_train,y_train)\n",
    "    preds_class = clf.predict(x_test)\n",
    "    acc=sum(y_test==preds_class)/len(y_test)\n",
    "#     print(\"    accuracy: \",acc)\n",
    "#     print(\"    f1: \",f1_score(y_test, preds_class))\n",
    "    print(\"    pr: \",precision_score(y_test, preds_class))\n",
    "#     print(\"    cm:\",confusion_matrix(y_test, preds_class))\n",
    "\n",
    "    print(\"    kaggle: \",sum(clf.predict(X_kag)))\n",
    "    if precision_score(y_test, preds_class)>.57:\n",
    "        print(\"yo\")\n",
    "        kags.append(clf.predict(X_kag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(kags).T.to_csv('aa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### category encodering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_all(df,df2,encoder_to_use,handle_missing='return_nan'):  #handle_missing='value'/'return_nan'\n",
    "    encoders_used={}\n",
    "    for col in encoder_to_use:\n",
    "        encoders_used[col]={}\n",
    "\n",
    "        if encoder_to_use[col]=='BinaryEncoder':\n",
    "            encoder=ce.BinaryEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing)\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder'\n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "        if encoder_to_use[col]=='CatBoostEncoder':\n",
    "            encoder=ce.CatBoostEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing)\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder'\n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "        if encoder_to_use[col]=='LeaveOneOutEncoder':\n",
    "            encoder=ce.LeaveOneOutEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing)\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder' \n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "        if encoder_to_use[col]=='OneHotEncoder':\n",
    "            encoder=ce.OneHotEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing,use_cat_names=True)\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder'\n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "        if encoder_to_use[col]=='OrdinalEncoder':\n",
    "            encoder=ce.OrdinalEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing)\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder' \n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "        if encoder_to_use[col]=='TargetEncoder':\n",
    "            encoder=ce.TargetEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing)\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder' \n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "\n",
    "\n",
    "        if encoder_to_use[col]=='WOEEncoder':\n",
    "            encoder=ce.WOEEncoder(cols=[col],return_df=1,drop_invariant=1,handle_missing=handle_missing)\n",
    "            encoder.fit(X=df,y=df['set_clicked'])\n",
    "            df=encoder.transform(df)\n",
    "            df2=encoder.transform(df2)\n",
    "            encoders_used[col]['encoder_type']='BackwardDifferenceEncoder' \n",
    "            encoders_used[col]['encoder_object']=encoder\n",
    "            \n",
    "            \n",
    "    return df,df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random encoders and imputers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "encoders_to_use=[\n",
    "{\n",
    "'query_detected_language':{'name':'TargetEncoder','min_samples_leaf':10, 'smoothing':4.0},\n",
    "'first_author_id':{'name':'TargetEncoder','min_samples_leaf':10, 'smoothing':4.0},\n",
    "'hour_request_received':{'name':'BaseNEncoder','base':2},\n",
    "'app_version':{'name':'TargetEncoder','min_samples_leaf':10, 'smoothing':4.0},\n",
    "'app_lang':{'name':'TargetEncoder','min_samples_leaf':10, 'smoothing':4.0},\n",
    "'country_by_ip':{'name':'TargetEncoder','min_samples_leaf':10, 'smoothing':4.0},\n",
    "'timezone_by_ip':{'name':'TargetEncoder','min_samples_leaf':10, 'smoothing':4.0},\n",
    "'local_hour_of_request':{'name':'BaseNEncoder','base':2},\n",
    "'recommendation_algorithm_id_used':{'name':'BaseNEncoder','base':2},\n",
    "'day_of_week':{'name':'BaseNEncoder','base':2},\n",
    "'month':{'name':'BaseNEncoder','base':2}\n",
    "},\n",
    "    {\n",
    "'query_detected_language':{'name':'MEstimateEncoder','randomized':True, 'sigma':0.1,'m':2},\n",
    "'first_author_id':{'name':'MEstimateEncoder','randomized':True, 'sigma':0.1,'m':2},\n",
    "'hour_request_received':{'name':'BaseNEncoder','base':2},\n",
    "'app_version':{'name':'MEstimateEncoder','randomized':True, 'sigma':0.1,'m':2},\n",
    "'app_lang':{'name':'MEstimateEncoder','randomized':True, 'sigma':0.1,'m':2},\n",
    "'country_by_ip':{'name':'MEstimateEncoder','randomized':True, 'sigma':0.1,'m':2},\n",
    "'timezone_by_ip':{'name':'MEstimateEncoder','randomized':True, 'sigma':0.1,'m':2},\n",
    "'local_hour_of_request':{'name':'BaseNEncoder','base':2},\n",
    "'recommendation_algorithm_id_used':{'name':'BaseNEncoder','base':2},\n",
    "'day_of_week':{'name':'BaseNEncoder','base':2},\n",
    "'month':{'name':'BaseNEncoder','base':2}\n",
    "},\n",
    "    {\n",
    "'query_detected_language':{'name':'LeaveOneOutEncoder','sigma':.3},\n",
    "'first_author_id':{'name':'LeaveOneOutEncoder','sigma':.3},\n",
    "'hour_request_received':{'name':'BaseNEncoder','base':2},\n",
    "'app_version':{'name':'LeaveOneOutEncoder','sigma':.3},\n",
    "'app_lang':{'name':'LeaveOneOutEncoder','sigma':.3},\n",
    "'country_by_ip':{'name':'LeaveOneOutEncoder','sigma':.3},\n",
    "'timezone_by_ip':{'name':'LeaveOneOutEncoder','sigma':.3},\n",
    "'local_hour_of_request':{'name':'BaseNEncoder','base':2},\n",
    "'recommendation_algorithm_id_used':{'name':'BaseNEncoder','base':2},\n",
    "'day_of_week':{'name':'BaseNEncoder','base':2},\n",
    "'month':{'name':'BaseNEncoder','base':2}\n",
    "},\n",
    "    {\n",
    "'query_detected_language':{'name':'LeaveOneOutEncoder','sigma':.1},\n",
    "'first_author_id':{'name':'LeaveOneOutEncoder','sigma':.1},\n",
    "'hour_request_received':{'name':'BaseNEncoder','base':2},\n",
    "'app_version':{'name':'LeaveOneOutEncoder','sigma':.1},\n",
    "'app_lang':{'name':'LeaveOneOutEncoder','sigma':.1},\n",
    "'country_by_ip':{'name':'LeaveOneOutEncoder','sigma':.1},\n",
    "'timezone_by_ip':{'name':'LeaveOneOutEncoder','sigma':.1},\n",
    "'local_hour_of_request':{'name':'BaseNEncoder','base':2},\n",
    "'recommendation_algorithm_id_used':{'name':'BaseNEncoder','base':2},\n",
    "'day_of_week':{'name':'BaseNEncoder','base':2},\n",
    "'month':{'name':'BaseNEncoder','base':2}\n",
    "},\n",
    "    {\n",
    "'query_detected_language':{'name':'WOEEncoder','randomized':True, 'sigma':0.1},\n",
    "'first_author_id':{'name':'WOEEncoder','randomized':True, 'sigma':0.1},\n",
    "'hour_request_received':{'name':'BaseNEncoder','base':2},\n",
    "'app_version':{'name':'WOEEncoder','randomized':True, 'sigma':0.1},\n",
    "'app_lang':{'name':'WOEEncoder','randomized':True, 'sigma':0.1},\n",
    "'country_by_ip':{'name':'WOEEncoder','randomized':True, 'sigma':0.1},\n",
    "'timezone_by_ip':{'name':'WOEEncoder','randomized':True, 'sigma':0.1},\n",
    "'local_hour_of_request':{'name':'BaseNEncoder','base':2},\n",
    "'recommendation_algorithm_id_used':{'name':'BaseNEncoder','base':2},\n",
    "'day_of_week':{'name':'BaseNEncoder','base':2},\n",
    "'month':{'name':'BaseNEncoder','base':2}\n",
    "},\n",
    "    {\n",
    "'query_detected_language':{'name':'WOEEncoder','randomized':True, 'sigma':0.3},\n",
    "'first_author_id':{'name':'WOEEncoder','randomized':True, 'sigma':0.3},\n",
    "'hour_request_received':{'name':'BaseNEncoder','base':2},\n",
    "'app_version':{'name':'WOEEncoder','randomized':True, 'sigma':0.3},\n",
    "'app_lang':{'name':'WOEEncoder','randomized':True, 'sigma':0.3},\n",
    "'country_by_ip':{'name':'WOEEncoder','randomized':True, 'sigma':0.3},\n",
    "'timezone_by_ip':{'name':'WOEEncoder','randomized':True, 'sigma':0.3},\n",
    "'local_hour_of_request':{'name':'BaseNEncoder','base':2},\n",
    "'recommendation_algorithm_id_used':{'name':'BaseNEncoder','base':2},\n",
    "'day_of_week':{'name':'BaseNEncoder','base':2},\n",
    "'month':{'name':'BaseNEncoder','base':2}\n",
    "}\n",
    "]\n",
    "    \n",
    "encoders_used={}\n",
    "\n",
    "def make_encoder_combinations(df):\n",
    "    return random.choice(encoders_to_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "encoding:  0  of  29\n",
      "imputing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nishant\\Anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:603: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-test-split:  0.29\n",
      "training models...\n",
      "    sum_of_kag1 DT 6.0\n",
      "    sum_of_kag2 Cat 6.0\n",
      "    sum_of_kag3 RF 18.0\n",
      "    sum_of_kag4 DT 5.0\n",
      "WARNING:tensorflow:From C:\\Users\\nishant\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\nishant\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "    sum_of_kag5 NN 41.632584\n",
      "    sum_of_kag6 LR 4.0\n",
      "    sum_of_kag7 XGB 5.0\n",
      "    sum_of_kag8 XGB 2.0\n",
      "    sum_of_kag9 XGB 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 195.9906244 Total: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sum_of_kag10 CAT 9.0\n",
      "training models after oversampling...\n",
      "    sum_of_kag11 DT 6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 195.9906244 Total: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sum_of_kag12 CAT 239.0\n",
      "    sum_of_kag13 RF 138.0\n",
      "    sum_of_kag14 DT 309.0\n",
      "    sum_of_kag15 NN 641.11096\n",
      "    sum_of_kag16 LR 73.0\n",
      "    sum_of_kag17 XGB 309.0\n",
      "    sum_of_kag18 XGB 257.0\n",
      "    sum_of_kag19 XGB 257.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 195.9906244 Total: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sum_of_kag20 CAT 199.0\n",
      "stacking...\n",
      "SUM_of_kag 8.0\n",
      "----------------------------------------------\n",
      "encoding:  1  of  29\n",
      "imputing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nishant\\Anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:603: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-test-split:  0.22\n",
      "training models...\n",
      "    sum_of_kag1 DT 7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 195.9906244 Total: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sum_of_kag2 Cat 6.0\n",
      "    sum_of_kag3 RF 7.0\n",
      "    sum_of_kag4 DT 6.0\n",
      "    sum_of_kag5 NN 45.43322\n",
      "    sum_of_kag6 LR 7.0\n",
      "    sum_of_kag7 XGB 6.0\n",
      "    sum_of_kag8 XGB 7.0\n",
      "    sum_of_kag9 XGB 7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 195.9906244 Total: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sum_of_kag10 CAT 6.0\n",
      "training models after oversampling...\n",
      "    sum_of_kag11 DT 7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 195.9906244 Total: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sum_of_kag12 CAT 229.0\n",
      "    sum_of_kag13 RF 54.0\n",
      "    sum_of_kag14 DT 423.0\n",
      "    sum_of_kag15 NN 581.1645\n",
      "    sum_of_kag16 LR 202.0\n",
      "    sum_of_kag17 XGB 423.0\n",
      "    sum_of_kag18 XGB 289.0\n",
      "    sum_of_kag19 XGB 289.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 195.9906244 Total: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sum_of_kag20 CAT 197.0\n",
      "stacking...\n",
      "SUM_of_kag 6.0\n",
      "----------------------------------------------\n",
      "encoding:  2  of  29\n",
      "imputing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nishant\\Anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:603: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-test-split:  0.24\n",
      "training models...\n",
      "    sum_of_kag1 DT 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 195.9906244 Total: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sum_of_kag2 Cat 7.0\n",
      "    sum_of_kag3 RF 11.0\n",
      "    sum_of_kag4 DT 6.0\n",
      "    sum_of_kag5 NN 39.708313\n",
      "    sum_of_kag6 LR 5.0\n",
      "    sum_of_kag7 XGB 6.0\n",
      "    sum_of_kag8 XGB 5.0\n",
      "    sum_of_kag9 XGB 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 195.9906244 Total: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sum_of_kag10 CAT 8.0\n",
      "training models after oversampling...\n",
      "    sum_of_kag11 DT 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 195.9906244 Total: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sum_of_kag12 CAT 146.0\n",
      "    sum_of_kag13 RF 55.0\n",
      "    sum_of_kag14 DT 374.0\n",
      "    sum_of_kag15 NN 582.6798\n",
      "    sum_of_kag16 LR 199.0\n",
      "    sum_of_kag17 XGB 374.0\n",
      "    sum_of_kag18 XGB 175.0\n",
      "    sum_of_kag19 XGB 175.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 195.9906244 Total: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sum_of_kag20 CAT 109.0\n",
      "stacking...\n",
      "SUM_of_kag 7.0\n",
      "----------------------------------------------\n",
      "encoding:  3  of  29\n",
      "imputing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nishant\\Anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:603: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-test-split:  0.25\n",
      "training models...\n",
      "    sum_of_kag1 DT 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 195.9906244 Total: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sum_of_kag2 Cat 9.0\n",
      "    sum_of_kag3 RF 11.0\n",
      "    sum_of_kag4 DT 6.0\n",
      "    sum_of_kag5 NN 27.248474\n",
      "    sum_of_kag6 LR 6.0\n",
      "    sum_of_kag7 XGB 6.0\n",
      "    sum_of_kag8 XGB 8.0\n",
      "    sum_of_kag9 XGB 8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 195.9906244 Total: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sum_of_kag10 CAT 5.0\n",
      "training models after oversampling...\n",
      "    sum_of_kag11 DT 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 195.9906244 Total: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sum_of_kag12 CAT 134.0\n",
      "    sum_of_kag13 RF 58.0\n",
      "    sum_of_kag14 DT 460.0\n",
      "    sum_of_kag15 NN 530.1073\n",
      "    sum_of_kag16 LR 147.0\n",
      "    sum_of_kag17 XGB 460.0\n",
      "    sum_of_kag18 XGB 175.0\n",
      "    sum_of_kag19 XGB 175.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 195.9906244 Total: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sum_of_kag20 CAT 115.0\n",
      "stacking...\n",
      "SUM_of_kag 5.0\n",
      "----------------------------------------------\n",
      "encoding:  4  of  29\n",
      "imputing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nishant\\Anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py:603: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-test-split:  0.19\n",
      "training models...\n",
      "    sum_of_kag1 DT 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 195.9906244 Total: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sum_of_kag2 Cat 7.0\n",
      "    sum_of_kag3 RF 13.0\n",
      "    sum_of_kag4 DT 4.0\n",
      "    sum_of_kag5 NN 35.484398\n",
      "    sum_of_kag6 LR 5.0\n",
      "    sum_of_kag7 XGB 4.0\n",
      "    sum_of_kag8 XGB 3.0\n",
      "    sum_of_kag9 XGB 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 195.9906244 Total: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sum_of_kag10 CAT 7.0\n",
      "training models after oversampling...\n",
      "    sum_of_kag11 DT 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 195.9906244 Total: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sum_of_kag12 CAT 145.0\n",
      "    sum_of_kag13 RF 56.0\n",
      "    sum_of_kag14 DT 322.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kag_pred_outer=[]\n",
    "\n",
    "enc=make_encoder_combinations(df_w_p)\n",
    "for xx,encoder_to_use in enumerate(enc):\n",
    "    \n",
    "    print(\"----------------------------------------------\")\n",
    "    print(\"encoding: \",xx,\" of \",len(enc))\n",
    "    df_w_encoded, df_kag_encoded=encode_all(df_w_p.copy(), df_kag_p.copy(),encoder_to_use)\n",
    "\n",
    "    print(\"imputing...\")\n",
    "\n",
    "#     random.make_imputer_combinations(df_w_p)\n",
    "    \n",
    "    est=IterativeImputer(random_state=0, estimator= KNeighborsRegressor())\n",
    "    est.fit(df_w_encoded)\n",
    "    X_w_im=est.transform(df_w_encoded)\n",
    "    X_kag_im=est.transform(df_kag_encoded)\n",
    "\n",
    "    X_w=X_w_im[:,:-1]\n",
    "    X_kag=X_kag_im[:,:-1]\n",
    "    y_w=df_w_p.iloc[:,-1]\n",
    "\n",
    "    x=random.randrange(15,40)/100\n",
    "    print(\"train-test-split: \",x)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_w, y_w, test_size=x, stratify=y_w)\n",
    "\n",
    "#     all models\n",
    "\n",
    "    test_preds=[]\n",
    "    kag_preds=[]\n",
    "\n",
    "    print(\"training models...\")\n",
    "    \n",
    "\n",
    "    model1 = tree.DecisionTreeClassifier(max_depth=4)\n",
    "    model1.fit(x_train,y_train)\n",
    "    test_pred1=model1.predict(x_test)\n",
    "    kag_pred1=model1.predict(X_kag)\n",
    "    test_preds.append(test_pred1)\n",
    "    kag_preds.append(kag_pred1)\n",
    "    print(\"    sum_of_kag1 DT\",kag_pred1.sum())\n",
    "\n",
    "    model2 = CatBoostClassifier(iterations=1500,learning_rate=.02,depth=4,task_type=\"GPU\",devices='0:2')\n",
    "    model2.fit(x_train,y_train, eval_set=(x_test,y_test),verbose=False)\n",
    "    test_pred2=model2.predict(x_test)\n",
    "    kag_pred2=model2.predict(X_kag)\n",
    "    test_preds.append(test_pred2)\n",
    "    kag_preds.append(kag_pred2)\n",
    "    print(\"    sum_of_kag2 Cat\",kag_pred2.sum())\n",
    "\n",
    "    model3 = RandomForestClassifier(n_estimators=800)\n",
    "    model3.fit(x_train,y_train)\n",
    "    test_pred3=model3.predict(x_test)\n",
    "    kag_pred3=model3.predict(X_kag)\n",
    "    test_preds.append(test_pred3)\n",
    "    kag_preds.append(kag_pred3)\n",
    "    print(\"    sum_of_kag3 RF\",kag_pred3.sum())\n",
    "\n",
    "    model4 = tree.DecisionTreeClassifier(max_depth=5)\n",
    "    model4.fit(x_train,y_train)\n",
    "    test_pred4=model4.predict(x_test)\n",
    "    kag_pred4=model4.predict(X_kag)\n",
    "    test_preds.append(test_pred4)\n",
    "    kag_preds.append(kag_pred4)\n",
    "    print(\"    sum_of_kag4 DT\",kag_pred4.sum())\n",
    "\n",
    "\n",
    "    # Neural network\n",
    "    model5 = Sequential()\n",
    "    model5.add(Dense(5, input_dim=x_train.shape[1], activation='relu'))\n",
    "    # model.add(Dense(12, activation='relu'))\n",
    "    model5.add(Dense(1, activation='sigmoid'))\n",
    "    model5.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model5.fit(x_train, y_train,validation_data = (x_test,y_test), epochs=10, batch_size=10, verbose=0)\n",
    "    test_pred5=model5.predict(x_test)\n",
    "    kag_pred5=model5.predict(X_kag)\n",
    "    test_preds.append(test_pred5)\n",
    "    kag_preds.append(kag_pred5)\n",
    "    print(\"    sum_of_kag5 NN\",kag_pred5.sum())\n",
    "\n",
    "    model6 = LogisticRegression(random_state=0, solver='lbfgs',max_iter=1000).fit(x_train, y_train)\n",
    "    model6.fit(x_train,y_train)\n",
    "    test_pred6=model6.predict(x_test)\n",
    "    kag_pred6=model6.predict(X_kag)\n",
    "    test_preds.append(test_pred6)\n",
    "    kag_preds.append(kag_pred6)\n",
    "    print(\"    sum_of_kag6 LR\",kag_pred6.sum())\n",
    "\n",
    "    model7 = xgb.XGBClassifier(depth=3,learning_rate=0.2,iterations=1000)\n",
    "    model7.fit(x_train,y_train)\n",
    "    test_pred7=model7.predict(x_test)\n",
    "    kag_pred7=model4.predict(X_kag)\n",
    "    test_preds.append(test_pred7)\n",
    "    kag_preds.append(kag_pred7)\n",
    "    print(\"    sum_of_kag7 XGB\",kag_pred7.sum())\n",
    "\n",
    "    model8 = xgb.XGBClassifier(depth=4,learning_rate=0.2,iterations=1000)\n",
    "    model8.fit(x_train,y_train)\n",
    "    test_pred8=model8.predict(x_test)\n",
    "    kag_pred8=model8.predict(X_kag)\n",
    "    test_preds.append(test_pred8)\n",
    "    kag_preds.append(kag_pred8)\n",
    "    print(\"    sum_of_kag8 XGB\",kag_pred8.sum())\n",
    "\n",
    "\n",
    "\n",
    "    model9 = xgb.XGBClassifier(depth=5,learning_rate=0.2,iterations=1000)\n",
    "    model9.fit(x_train,y_train)\n",
    "    test_pred9=model9.predict(x_test)\n",
    "    kag_pred9=model9.predict(X_kag)\n",
    "    test_preds.append(test_pred9)\n",
    "    kag_preds.append(kag_pred9)\n",
    "    print(\"    sum_of_kag9 XGB\",kag_pred9.sum())\n",
    "\n",
    "\n",
    "    model10 = CatBoostClassifier(iterations=1500,learning_rate=.02,depth=5,task_type=\"GPU\",devices='0:2')\n",
    "    model10.fit(x_train,y_train, eval_set=(x_test,y_test),verbose=False)\n",
    "    test_pred10=model10.predict(x_test)\n",
    "    kag_pred10=model10.predict(X_kag)\n",
    "    test_preds.append(test_pred10)\n",
    "    kag_preds.append(kag_pred10)\n",
    "    print(\"    sum_of_kag10 CAT\",kag_pred10.sum())\n",
    "\n",
    "    #oversampling\n",
    "    sampler = SMOTE(ratio=0.3)\n",
    "    x_train, y_train = sampler.fit_sample(x_train, y_train)\n",
    "\n",
    "    print(\"training models after oversampling...\")\n",
    "    model11 = tree.DecisionTreeClassifier(max_depth=4)\n",
    "    model11.fit(x_train,y_train)\n",
    "    test_pred11=model11.predict(x_test)\n",
    "    kag_pred11=model1.predict(X_kag)\n",
    "    test_preds.append(test_pred11)\n",
    "    kag_preds.append(kag_pred11)\n",
    "    print(\"    sum_of_kag11 DT\",kag_pred11.sum())\n",
    "\n",
    "    model12 = CatBoostClassifier(iterations=1500,learning_rate=.02,depth=4,task_type=\"GPU\",devices='0:2')\n",
    "    model12.fit(x_train,y_train, eval_set=(x_test,y_test),verbose=False)\n",
    "    test_pred12=model12.predict(x_test)\n",
    "    kag_pred12=model12.predict(X_kag)\n",
    "    test_preds.append(test_pred12)\n",
    "    kag_preds.append(kag_pred12)\n",
    "    print(\"    sum_of_kag12 CAT\",kag_pred12.sum())\n",
    "\n",
    "    model13 = RandomForestClassifier(n_estimators=400)\n",
    "    model13.fit(x_train,y_train)\n",
    "    test_pred13=model13.predict(x_test)\n",
    "    kag_pred13=model13.predict(X_kag)\n",
    "    test_preds.append(test_pred13)\n",
    "    kag_preds.append(kag_pred13)\n",
    "    print(\"    sum_of_kag13 RF\",kag_pred13.sum())\n",
    "\n",
    "    model14 = tree.DecisionTreeClassifier(max_depth=5)\n",
    "    model14.fit(x_train,y_train)\n",
    "    test_pred14=model14.predict(x_test)\n",
    "    kag_pred14=model14.predict(X_kag)\n",
    "    test_preds.append(test_pred14)\n",
    "    kag_preds.append(kag_pred14)\n",
    "    print(\"    sum_of_kag14 DT\",kag_pred14.sum())\n",
    "\n",
    "    # Neural network\n",
    "    model15 = Sequential()\n",
    "    model15.add(Dense(5, input_dim=x_train.shape[1], activation='relu'))\n",
    "    # model1.add(Dense(12, activation='relu'))\n",
    "    model15.add(Dense(1, activation='sigmoid'))\n",
    "    model15.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model15.fit(x_train, y_train,validation_data = (x_test,y_test), epochs=10, batch_size=10, verbose=0)\n",
    "    test_pred15=model15.predict(x_test)\n",
    "    kag_pred15=model15.predict(X_kag)\n",
    "    test_preds.append(test_pred15)\n",
    "    kag_preds.append(kag_pred15)\n",
    "    print(\"    sum_of_kag15 NN\",kag_pred15.sum())\n",
    "\n",
    "    model16 = LogisticRegression(random_state=0, solver='lbfgs',max_iter=1000).fit(x_train, y_train)\n",
    "    model16.fit(x_train,y_train)\n",
    "    test_pred16=model16.predict(x_test)\n",
    "    kag_pred16=model16.predict(X_kag)\n",
    "    test_preds.append(test_pred16)\n",
    "    kag_preds.append(kag_pred16)\n",
    "    print(\"    sum_of_kag16 LR\",kag_pred16.sum())\n",
    "\n",
    "    model17 = xgb.XGBClassifier(depth=3,learning_rate=0.2,iterations=1000)\n",
    "    model17.fit(x_train,y_train)\n",
    "    test_pred17=model17.predict(x_test)\n",
    "    kag_pred17=model14.predict(X_kag)\n",
    "    test_preds.append(test_pred17)\n",
    "    kag_preds.append(kag_pred17)\n",
    "    print(\"    sum_of_kag17 XGB\",kag_pred17.sum())\n",
    "\n",
    "    model18 = xgb.XGBClassifier(depth=4,learning_rate=0.2,iterations=1000)\n",
    "    model18.fit(x_train,y_train)\n",
    "    test_pred18=model18.predict(x_test)\n",
    "    kag_pred18=model18.predict(X_kag)\n",
    "    test_preds.append(test_pred18)\n",
    "    kag_preds.append(kag_pred18)\n",
    "    print(\"    sum_of_kag18 XGB\",kag_pred18.sum())\n",
    "\n",
    "\n",
    "    model19 = xgb.XGBClassifier(depth=5,learning_rate=0.2,iterations=1000)\n",
    "    model19.fit(x_train,y_train)\n",
    "    test_pred19=model19.predict(x_test)\n",
    "    kag_pred19=model19.predict(X_kag)\n",
    "    test_preds.append(test_pred19)\n",
    "    kag_preds.append(kag_pred19)\n",
    "    print(\"    sum_of_kag19 XGB\",kag_pred19.sum())\n",
    "\n",
    "    model20 = CatBoostClassifier(iterations=1500,learning_rate=.02,depth=5,task_type=\"GPU\",devices='0:2')\n",
    "    model20.fit(x_train,y_train, eval_set=(x_test,y_test),verbose=False)\n",
    "    test_pred20=model20.predict(x_test)\n",
    "    kag_pred20=model20.predict(X_kag)\n",
    "    test_preds.append(test_pred20)\n",
    "    kag_preds.append(kag_pred20)\n",
    "    print(\"    sum_of_kag20 CAT\",kag_pred20.sum())\n",
    "\n",
    "\n",
    "    # stack\n",
    "    print(\"stacking...\")\n",
    "    test_df=pd.DataFrame(test_preds).T\n",
    "    kag_df=pd.DataFrame(kag_preds).T\n",
    "\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(test_df,y_test)\n",
    "    kag_preds=model.predict(kag_df)\n",
    "    print(\"SUM_of_kag\",kag_preds.sum())\n",
    "    \n",
    "    kag_pred_outer.append(kag_preds)\n",
    "\n",
    "    \n",
    "pd.DataFrame(kag_pred_outer).T.to_csv('aa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
